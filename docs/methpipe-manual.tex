\documentclass[10pt]{article}

\usepackage{times,fullpage,graphicx,amsmath, subfigure}
\usepackage[pdfborder={0 0 0}]{hyperref}

\title{The Smithlab DNA Methylation Data Analysis Pipeline (MethPipe)} 
\author{Qiang Song \and Benjamin Decato \and Michael Kessler \and Fang Fang
\and Jenny Qu \and Tyler Garvin \and Meng Zhou \and Andrew Smith}

\newcommand{\meth}{\texttt{methpipe}}

%%%%
%%%% NOTE: use the following appropriately so that we can change the
%%%% individual types later.
%%%%
%%
%% For program names
\newcommand{\prog}[1]{\texttt{#1}}
%% For file names
\newcommand{\fn}[1]{\texttt{#1}}
%% For literals
\newcommand{\lit}[1]{\texttt{#1}}
%% For program options
\newcommand{\op}[1]{\texttt{#1}}

\begin{document}

\maketitle


The \meth{} software package is a comprehensive pipeline and set of
tools for analyzing whole genome bisulfite sequencing data
(BS-seq). This manual explains the stages in our pipeline, how to use
the analysis tools, and how to modify the pipeline for your specific
context.

\tableofcontents

\newpage


\section{Assumptions}

Our pipeline was designed to run in a cluster computing context, with
many processing nodes available, and a job submission system like PBS
or SGE. Much of this analysis is computationally intensive. We assume
that individual nodes will have several GB of memory available for
processing. Typically the data we deal with amounts to a minimum of
100GB for a mammalian methylome at 10x coverage. Intermediate files
may cause this amount to more than double during execution of the
pipeline, and likely at the end of the pipeline the total size of
files will amount to almost double the size of the raw data.

Users are assumed to be quite familiar with UNIX/Linux and related
concepts ({\em e.g.} building software from source, using the command
line, shell environment variables, etc.).

It is also critical that users are familiar with BS-seq experiments,
especially the bisulfite conversion reaction, and how this affects
what we observe in the sequenced reads. This is especially important
 if paired-end sequencing is used. If you do not understand these
 concepts, you will likely run into major problems trying to customize our pipeline.

\section{Methylome construction}

\subsection{Mapping reads}
\label{sec:mapping}

% \begin{description}
% \item[rmapbs.cpp]
% This program takes fastq file as input and output mapped read file
% \end{description}

During bisulfite treatment, unmethylated cytosines in the original DNA
sequences are converted to uracils, which are then incorporated as
thymines (T) during PCR amplification. These PCR products are referred
to as T-rich sequences as a result of their high thymine
constitution. With paired-end sequencing experiments, the compliments
of these T-rich sequences are also sequenced.  These complimentary
sequences have high adenosine (A) constitution (A is the complimentary
base pair of T), and are referred to as A-rich sequences. Mapping
consists of finding sequence similarity, based on context specific
criteria, between these short sequences, or reads, and an orthologous
reference genome.  When mapping T-rich reads to the
reference genome, either a cytosine (C) or a thymine (T) in a read is
considered a valid match for a cytosine in the reference genome. For
A-rich reads, an adenine or a guanine is considered a valid match for
a guanine in the reference genome. The mapping of reads to the
reference genome by \prog{rmapbs} is described below. If you choose
to map reads with a different tool, make sure that your post-mapping
files are appropriately formatted for the next components of the
\meth{} pipeline (necessary file formats for each step are covered in
the corresponding sections).  The default behavior of rmapbs is to
assume that reads are T-rich and map accordingly. To change the
mapping to suit A-rich reads, add the \op{-A} option.

\paragraph{Input and output file formats:} We assume that the original
data is a set of sequenced read files, typically as produced by
Illumina sequencing. These are FASTQ format files, and can be quite
large. After the reads are mapped, these files are not used by our
pipeline.  The reference genome should be a folder containing an
individual FASTA (named like \fn{*.fa}) file for each chromosome to
maximize memory efficiency.

The mapped reads files (\fn{*.mr} suffix) that result from the
previous steps should consist of eight columns of data. The first six
columns are the traditional components of a BED file (chromosome,
start, end, read name, number of mismatches, strand), while the last
two columns consist of sequence and quality scores respectively. These
mapped reads files will be the input files for the following two
\meth{} components, \prog{bsrate} and \prog{methcounts}.

\paragraph{Decompressing and isolating paired-end reads:}
Sometimes paired-end reads are stored in the same FASTQ file.  Because
we treat these paired ends differently, they must be separated into
two files and run through \prog{rmapbs} with different parameters.

If your data is compressed as a Sequenced Read Archive, or SRA file, you can
decompress and split paired-end reads into two files at the same time using
 \prog{fastq-dump}, which is a program included in the \prog{sra-toolkit} 
package, available for most unix systems.  Below is an example of using
\prog{fastq-dump} to decompress and separate FASTQ data by end:
\begin{verbatim}
$ ./fastq-dump --split-3 Human_ESC.sra
\end{verbatim}

If you have a FASTQ file not compressed in SRA format, you can split paired ends
into two separate files by running the following commands:
\begin{verbatim}
$ sed -ne '1~8{N;N;N;p}' *.fastq > *_1.fastq
$ sed -ne '4~8{N;N;N;p}' *.fastq > *_2.fastq
\end{verbatim}

\paragraph{Sequencing adaptors:}
These are a problem in any sequencing experiment with short fragments
relative to the lengths of reads.  \prog{rmapbs} identifies sequences at the
ends of reads greater than 10bp belonging to sequencing adaptors and
converts them to Ns to avoid potential mapping problems.

Adaptor sequences must be supplied to \prog{rmapbs} through the \op{-C}
option.  Keep in mind that if the adaptor sequence provided to you for
the second paired end is displayed from 5' to 3', you will need to
provide the reverse complement of the sequence to \prog{rmapbs}.

% In the last versions of the manual there was a section regarding "Trim
% adapters" which was very useful. But I think it needs more
% explanation.  I think it will be very useful to provide two examples
% for SE and PE reads

% for instance, if we assume that Illumina adapters are

% SE adapter
% 5' ACACTCTTTCCCTACACGACGCTCTTCCGATCT
% PE1 adapter
% 5' ACACTCTTTCCCTACACGACGCTCTTCCGATCT
% PE2 adapter
% 5' CGGTCTCGGCATTCCTGCTGAACCGCTCTTCCGATCT

% what would be the right direction for adapters in the following commands
% for single-end:

% ./rmapbs -C (which direction?) -c hg18 -o results.mr sequence.fastq

% for PE mate 1:

% ./rmapbs -C ??? -c hg18 -o results.mr s_1_1_sequence.fastq

% for PE mate 2:

% ./rmapbs -C ??? -c hg18 -o results.mr s_1_2_sequence.fastq

% Because as I told you previously, when I wanted to cut adapters before
% mapping (e.g. ./cutadapt) So, if we assume SE:

% ./cutadapt -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT .fastq

% Also, in this case I suppose BS-seq library (Schubeler's data
% "SRR299056.fastq") is directional, I only ever expect to sequence into
% the second adapter which is 5' ACACTCTTTCCCTACACGACGCTCTTCCGATCT. I
% needed to reverse complement the sequence because that would be the
% way it is read during the sequencing.

\paragraph{Single-end reads:}
When working with data from a single-end sequencing experiment, you
will have T-rich reads only. \prog{rmapbs} expects T-rich reads as a
default and so you do not have use the \op{-A} option to change
mapping parameters. Execute the following command to map all of your
single-end reads with \prog{rmapbs}:
\begin{verbatim}
$ ./rmapbs -c hg18 -o Human_NHFF.mr Human_NHFF.fastq
\end{verbatim}

\paragraph{Paired-end reads:}
When working with data from a paired-end sequencing experiment, you
will have T-rich and A-rich reads. T-rich reads are often kept in
files labeled with an ``\_1'' and A-rich reads are often kept in files
labeled with an ``\_2''.  T-rich reads are sometimes referred to as
$5^{\prime}$ reads or mate 1 and A-rich reads are sometimes referred
to $3^{\prime}$ reads or mate 2. We assume that the T-rich file and
the A-rich contain the same number of reads, and each pair of mates
occupy the same lines in their respective files. We will follow this
convention throughout the manual and strongly suggest that you do the
same. The program \prog{rmapbs-pe} program is used to map T-rich reads
and A-rich reads simultaneously. Run the following command to map two
reads files from a paired-end sequencing experiment:
\begin{verbatim}
$ ./rmapbs-pe -c hg18 -o Human_ESC.mr Human_ESC_1.fastq Human_ESC_2.fastq
\end{verbatim}

In brief, what happens internally in \prog{rmapbs-pe} is as follows.
\prog{rmapbs-pe} finds candidate mapping locations for a T-rich mate
with CG-wildcard mapping, and candidate mapping locations for the
corresponding A-rich mate with AG-wildcard mapping. If two candidate
mapping locations of the pair of mates are within certain distance in
the same chromosome and strand and with correct orientation, the two
mates are combined into a single read (after reverse complement of the
A-rich mate), referred to as a fragment. The overlapping region
between the two mates, if any, is included once, and the gap region
between them, if any, is filled with Ns. The parameter \op{-L} to
\prog{rmapbs-pe} indicates the maximum size of fragments to allow to
be merged. Here the fragment size is the sum of the read lengths at
both ends, plus whatever distance is between them. So this is the
length of the original molecule that was sequenced, excluding the
sequencing adaptors. It is possible for a given read pair that the
molecule was shorter than twice the length of the reads, in which case
the ends of the mates will overlap, and so in the merged fragment will
only be included once. Also, it is possible that the entire molecule
was shorter than the length of even one of the mates, in which case
the merged fragment will be shorter than either of the read ends. If
the two mates cannot be merged because they are mapped to different
chromosomes or different strand, or they are far away from each other,
\prog{rmapbs-pe} will throw each mate individually if its mapping
position is unambiguous.

%%%%%%% the following way are deprecated because of rmapbs-pe

% \prog{rmapbs} expects T-rich reads as a default and so for
% paired-end sequencing data, you must run \prog{rmapbs} twice; once
% for T-rich reads and once for A-rich reads. Execute the following
% commands to map of your paired-end reads with \prog{rmapbs}; one
% command for T-rich reads, and one for A-rich reads.
% \begin{verbatim}
% $ ./rmapbs -c hg18 -o Human_ESC_1.mr Human_ESC_1.fastq
% \end{verbatim}
% An example command for the second end (previously named like
% \fn{s\_1\_2\_sequence.txt} from Illumina), which will contain all
% A-rich reads:
% \begin{verbatim}
% $ ./rmapbs -c hg18 -o Human_ESC_2.mr -A Human_ESC_2.fastq
% \end{verbatim}

% \paragraph{Merging paired-end reads:}
% As previously noted, paired-end sequencing produces both T-rich reads
% and A-rich reads. Since methylation is estimated from the counts of T's
% and C's (that map to C's in the reference genome), and A-rich reads
% contain methylation information in the form of A and G counts (that
% map to G's in the reference genome), A-rich reads must be transformed
% into T-rich reads by reverse complementation. This allows methylation
% information to be derived from the T's and C's in the new T-rich reads
% that correspond to the A's and G's from the A-rich reads that were
% reverse complemented.

% A double counting error could arise if overlapping regions exist
% between any two mates. To avoid this bias and maintain all other
% important information, we merge read ends that are mapped sufficiently
% closely and in the proper relative orientations. Any overlapping portion
% is eliminated. Reverse complementation of A-rich reads, switching
% the strand to which they mapped, and eliminating overlapping regions is
% done by the program \prog{clipmates}. This tool works on mapped reads
% files that been sorted by read id (read name). The following command
% is an example of how to sort your mapped reads files with the Unix
% \prog{sort} command before inputting them into \prog{clipmates}.
% \begin{verbatim}
% $ LC_ALL=C sort -k 4 -o Human_ESC_1.mr.name_srtd Human_ESC_1.mr
% $ LC_ALL=C sort -k 4 -o Human_ESC_2.mr.name_srtd Human_ESC_2.mr
% \end{verbatim}
% The names of corresponding mates must only differ by the last
% character (which should be a 1 for T-rich reads and 2 for A-rich
% reads). If mates exist and are mapped correctly (to the same
% chromosome, to correct strands, with correct orientation) within a
% certain distance from each other (as specified by the \op{-L} option),
% then the mates are combined into a single read, referred to as a
% fragment, with Ns filling any existing gaps between mates. If mates
% could not be matched, then each mate is present in the output
% file by default.  Unmatched reads can be thrown out using the \op{-t}
% option.  \prog{clipmates} takes two input files in mapped reads format:
% one with T-rich mapped reads (\op{-T} option) and the other with A-rich
% mapped reads (\op{-A} option). Execute the following command to run the
% \prog{clipmates} component of \meth{} on T-rich and A-rich reads
% (mates 1 and 2).
% \begin{verbatim}
% $ ./clipmates -L 1000 -S Human_ESC.clipstats -o Human_ESC_clipped.mr \
%               -T Human_ESC_1.mr.name_srtd -A Human_ESC_2.mr.name_srtd
% \end{verbatim}
% The parameter \op{-L} to \prog{clipmates} indicates the maximum size
% of fragments to allow to be merged. Here the fragment size is the sum
% of the read lengths at both ends, plus whatever distance is between
% them. So this is the length of the original molecule that was
% sequenced, excluding the sequencing adaptors. It is possible for a
% given read pair that the molecule was shorter than twice the length of
% the reads, in which case the ends of the mates will overlap, and so in
% the merged fragment will only be included once. Also, it is possible
% that the entire molecule was shorter than the length of even one of
% the mates, in which case the merged fragment will be shorter than
% either of the read ends.

% Typically the Illumina software adds a suffix of \lit{\#0/1} to the
% names of the first-end reads and \lit{\#0/2} to the names of the
% second-end reads. The \prog{clipmates} program must match up the
% identical reads based on these names, and so by default ignores the
% final character of the read names. However, some reads have their
% names changed, for example many downloaded from SRA have identical
% names for both first-end and second-end reads. In that case, the
% entire read name must be matched, and so \prog{clipmates} has an
% option to accomplish this:
% \begin{verbatim}
% $ ./clipmates -s 0 -L 1000 -S SRA_reads.clipstats -o SRA_reads_clipped.mr \
%               -T SRA_reads_1.mr.name_srtd -A SRA_reads_2.mr.name_srtd
% \end{verbatim}
% The argument \lit{-s} is used to indicate that the suffix of the read
% names to be ignored when matching up mates has length 0, so compare
% the entire read name.

\paragraph{Mapping reads in a large file:} 
Mapping reads often takes a while, and mapping reads from BS-seq takes
even longer. It usually take quite a long time to map reads from a
single large file with tens of millions of reads. If you have access
to a cluster, one strategy is to launch multiple jobs, each working on
a subset of reads simultaneously, and finally combine their output. I
will typically map 3M reads at a time, and this takes at most 1.5GB of
memory for the human genome and with 100nt reads. If all computing
nodes can read the large input file (for example, through NFS), you
may use the option \op{-T s} and \op{-N n} to instruct \prog{rmapbs}
or \prog{rmapbs-pe} to map \op{n} reads starting from the $s^{th}$
reads in the input file. For example, with following command
\begin{verbatim}
$ ./rmapbs -c hg18 -o Human_NHFF.mr Human_NHFF.fastq -T 1 -N 1000000
\end{verbatim}
\prog{rmapbs} will map the first million reads in the input file.   
  
If each node can only access its local storage, dividing the set of
reads to into $k$ equal sized smaller reads files, and mapping these
all simultaneously on multiple nodes, will make the mapping finish
about $k$ times faster.  The unix \prog{split} command is good for
dividing the reads into smaller parts. The following BASH commands
will take a directory named \fn{reads} containing Illumina sequenced
reads files, and split them into files containing at most 3M reads:
\begin{verbatim}
$ mkdir reads_split
$ for i in reads/*.txt; do \
    split -a 3 -d -l 12000000 ${i} reads_split/$(basename $i); done
\end{verbatim}
Notice that the number of lines per split file is 12M, since we want
3M reads, and there are 4 lines per read. If you split the reads like
this, you will need to ``unsplit'' them after the mapping is done. Not
a problem, just use the \prog{cat} command.

\paragraph{Alternative mappers:}
\label{sec:alternative-mappers}
In addition to \prog{rmapbs} described above, users may also wish
to process raw reads using alternative mapping algorithms, including
BSSeeker, which uses a three nucleotide alphabet strategy, and BSMAP,
which allows for gaps during mapping. The program \prog{to-mr} is used
to convert the output from those mappers to the \fn{*.mr} format used in
our pipeline. To convert BSMAP mapped read file in .bam format, run
\begin{verbatim}
$ ./to-mr -o Human_NHFF.mr -m bsmap Human_NHFF.bam
\end{verbatim}
where the option \op{-m} specifies that the original mapper is
BSMAP. To obtain a list of alternative mappers supported by our
converter, run \prog{to-mr} without any options. 

\subsection{Merging libraries and removing duplicates}

Before calculating methylation level, you should now remove read
duplicates, or reads that were mapped to identical genomic
locations. These reads are most likely the results of PCR
over-amplication rather than true representations of distinct DNA
molecules. The program \prog{duplicate-remover} aims to remove such
duplicates. It collects duplicate reads and/or fragments that have
identical sequences and are mapped to the same genomic location (same
chromosome, same start and end, and same strand), and chooses a random
one to be the representative of the original DNA sequence.

\prog{duplicate-remover} can take reads sorted by (chrom, start, end,
strand). If the reads in the input file are not sorted, run the
following sort command:
\begin{verbatim}
$ LC_ALL=C sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 \
       -o Human_ESC.mr.sorted_start Human_ESC.mr
$ LC_ALL=C sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 \
       -o Human_NHFF.mr.sorted_start Human_NHFF.mr
\end{verbatim}
Next, execute the following command to remove duplicate reads:
\begin{verbatim}
$ ./duplicate-remover -S Human_ESC_dremove_stat.txt \
                      -o Human_ESC.mr.dremove Human_ESC.mr.sorted_start
$ ./duplicate-remover -S Human_NHFF_dremove_stat.txt \
                      -o Human_NHFF.mr.dremove Human_NHFF.mr.sorted_start
\end{verbatim}
The duplicate-removal correction should be done on a per-library
basis, i.e, one should pool all reads from multiple runs or lanes
sequenced from the same library and remove duplicates. The reads from
distinct libraries can be simply pooled without any correction as the
reads from each library are originated from distinct DNA
fragments. Please refer to \ref{sec:organ-datas-with} for recommended
practices to organize a project with multiple runs and/or libraries.


\subsection{Estimating bisulfite conversion rate}
\label{sec:estim-busilf-conv}

Unmethylated cytosines in DNA fragments are converted to uracils by
sodium bisulfite treatment. As these fragments are amplified, the
uracils are converted to thymines and so unmethylated Cs are
ultimately read as Ts (barring error). Despite its high fidelity,
bisulfite conversion of C to T does have some inherent failure rate,
depending on the bisulfite kit used, reagent concentration, time of
treatment, etc., and these factors may impact the success rate of the
reaction. Therefore, the bisulfite conversion rate, defined as the
rate at which unmethylated cytosines in the sample appear as Ts in the
sequenced reads, should be measured and should be very high ({\em
  e.g.} $>0.99$) for the experiment to be considered a success.

Measuring the bisulfite conversion rate this way requires some kind of
control set of genomic cytosines not believed to be methylated. Three
options are (1) to spike in some DNA known not to be methylated, such
as a Lambda virus, (2) to use the human mitochondrial genome, which
is known to be entirely unmethylated, or chloroplast genomes
which are believed not to be methylated, or (3) to use non-CpG cytosines
which are believed to be almost completely unmethylated in most
mammalian cells. In general the procedure is to identify the positions
in reads that correspond to these presumed unmethylated cytosines,
then compute the ratio of C to (C + T) at these positions. If the
bisulfite reaction was perfect, then this ratio should be very close
to 1, and if there is no bisulfite treatment, then this ratio should
be close to 0.

The program \prog{bsrate} will estimate the bisulfite conversion rate
in this way. Assuming method (3) from the above paragraph of measuring
conversion rate at non-CpG cytosines in a mammalian methylome, the
following command will estimate the conversion rate.
\begin{verbatim}
$ ./bsrate -c hg18 -o Human_ESC.bsrate Human_ESC.mr
$ ./bsrate -c hg18 -o Human_NHFF.bsrate Human_NHFF.mr
\end{verbatim}
The \prog{bsrate} program requires that the input be sorted so that
reads mapping to the same chromosome are contiguous. The first several
lines of the output might look like the following:
{\small{%%
\begin{verbatim}
OVERALL CONVERSION RATE = 0.994141
POS CONVERSION RATE = 0.994166  832349
NEG CONVERSION RATE = 0.994116  825919
BASE PTOT  PCONV PRATE   NTOT  NCONV NRATE   BTHTOT BTHCONV BTHRATE ERR ALL    ERRRATE
1    8964  8813  0.9831  9024  8865  0.9823  17988  17678   0.9827  95  18083  0.0052
2    7394  7305  0.9879  7263  7183  0.9889  14657  14488   0.9884  100 14757  0.0067
3    8530  8442  0.9896  8323  8232  0.9890  16853  16674   0.9893  98  16951  0.0057
4    8884  8814  0.9921  8737  8664  0.9916  17621  17478   0.9918  76  17697  0.0042
5    8658  8596  0.9928  8872  8809  0.9929  17530  17405   0.9928  70  17600  0.0039
6    9280  9218  0.9933  9225  9177  0.9948  18505  18395   0.9940  59  18564  0.0031
7    9165  9117  0.9947  9043  8981  0.9931  18208  18098   0.9939  69  18277  0.0037
8    9323  9268  0.9941  9370  9314  0.9940  18693  18582   0.9940  55  18748  0.0029
9    9280  9228  0.9944  9192  9154  0.9958  18472  18382   0.9951  52  18524  0.0028
10   9193  9143  0.9945  9039  8979  0.9933  18232  18122   0.9939  66  18298  0.0036
\end{verbatim}%%
}}

\noindent
The above example is based on a very small number of mapped reads in
order to make the output fit the width of this page.  The first thing
to notice is that the conversion rate is computed separately for each
strand. The information is presented separately because this is often
a good way to see when some problem has occurred in the context of
paired-end reads. If the conversion rate looks significantly different
between the two strands, then we would go back and look for a mistake
that has been made at an earlier stage in the pipeline. The first 3
lines in the output indicate the overall conversion rate, the
conversion rate for positive strand mappers, and the conversion rate
for negative strand mappers. The total number of nucleotides used
({\em e.g.} all C+T mapping over genomic non-CpG C's for method (3)) is
given for positive and negative strand conversion rate computation,
and if everything has worked up to this point these two numbers should
be very similar. The 4th line gives column labels for a table showing
conversion rate at each position in the reads.  The labels PTOT, PCONV
and PRATE give the total nucleotides used, the number converted, and
the ratio of those two, for the positive-strand mappers. The
corresponding numbers are also given for negative strand mappers
(NTOT, NCONV, NRATE) and combined (BTH). The sequencing error rate is
also shown for each position, though this is an underestimate because
we assume at these genomic sites any read with either a C or a T
contains no error.

If you are using reads from an unmethylated spike-in or reads mapping
to mitochondria, then there is an option to use all Cs, including those
at CpG sites:
\begin{verbatim}
$ grep ^chrM Human_ESC.mr > Human_ESC.mr.chrM
$ ./bsrate -N -c chrM.fa -o Human_ESC.bsrate Human_ESC.mr.chrM
\end{verbatim}

After completing bisulfite conversion rate analysis, remember to
remove any control reads not naturally occurring in the sample
(lambda virus, mitochondrial DNA from another organism, etc.)
before continuing. The output from two different runs of \prog{bsrate}
can be merged using the program \prog{merge-bsrate}.

\subsection{Computing single-site methylation levels}
\label{sec:estim-methyl-freq}

The \prog{methcounts} program takes the mapped reads and produces the
methylation level at each genomic cytosine, with the option to produce
only levels for CpG-context cytosines.
While most DNA methylation exists in the CpG context, cytosines in
other sequence contexts, such as CXG or CHH (where H denotes adenines,
thymines, or cytosines and X denotes adenines or thymines) may also be
methylated. Non-CpG methylation occurs most frequently in plant genomes 
and pluripotent mammalian cells such as embryonic stem cells. This type
of methylation is asymmetric since the cytosines on the complementary 
strand do not necessarily have the same methylation status.

The input is in MappedRead format, and the reads should be sorted
according to (chrom, end, start, strand). If your reads are not
sorted, run:
\begin{verbatim}
$ LC_ALL=C sort -k 1,1 -k 3,3n -k 2,2n -k 6,6 \
       -o Human_ESC.mr.sorted_end_first Human_ESC.mr
\end{verbatim}

Since \prog{methcounts} can only take one input file, if you have
multiple you can merge them using the \op{-m} option to the
\prog{sort} program:

\begin{verbatim}
$ LC_ALL=C sort -m -k 1,1 -k 3,3n -k 2,2n -k 6,6 \
       -o Human_ESC.mr.sorted_end_first Human_ESC.mr.1 Human_ESC.mr.2
\end{verbatim}

\paragraph{Running \prog{methcounts}:}
The methylation level for every cytosine site at single base resolution
is estimated as a probability based on the ratio of methylated to total
reads mapped to that loci. Because not all DNA methylation contexts are
symmetric, methylation levels are produced for both strands and can be
analyzed separately. To compute methylation levels at each
cytosine site along the genome you can use the following command:

\begin{verbatim}
$ ./methcounts -c hg38 -o Human_ESC.meth \
               -S Human_ESC.methstats Human_ESC.mr
$ ./methcounts -c hg18 -o Human_NHFF.meth \
               -S Human_NHFF.methstats Human_NHFF.mr
\end{verbatim}

The argument \op{-c} gives the filename of the genome sequence or the
directory that contains one FASTA format file for each chromosome. By
default \prog{methcounts} identifies these chromosome files by the
extension \fn{.fa}. Importantly, the ``name'' line in each chromosome
file must be the character \lit{>} followed by the same name that
identifies that chromosome in the mapped read output (the \fn{.mr}
files).

The output file contains one line per cytosine site. The first column is
the chromosome. The second is the location of the cytosine. The 3rd column indicates the strand, which can be either \lit{+} or \lit{-}. The 4th
column is the sequence context of that site, followed by an x if the
site has mutated in the sample away from the reference genome. The 5th 
column is the estimated methylation level, equal to the number of Cs in 
reads at position corresponding to the site, divided by the sum of the Cs 
and Ts mapping to that position. The final column is number of reads
overlapping with that site.

Note that because \prog{methcounts} produces a file containing one line
for every cytosine in the genome, the file can get quite large. For
reference assembly mm10, the output is approximately 25GB. The \op{-n}
option produces methylation data for CpG context sites only, and for
mm10 this produces an output file that is approximately 1GB. It is
recommended that users allocate at least 8GB of memory when running
\prog{methcounts}.

To examine the methylation status of cytosines a particular sequence
context, one may use the \prog{grep} command to filter those lines
based on the fourth column. For example, in order to pull out all
cytosines within the CHG context, run the following:

\begin{verbatim}
$ grep CHG Human_ESC_All.meth > Human_ESC_CHG.meth
\end{verbatim}

Our convention is to name \prog{methcounts} output with all cytosines
like \fn{*\_All.meth}, with CHG like \fn{*\_CHG.meth} and
with CHH like \fn{*\_CHH.meth}.

\paragraph{Merging methcounts files from multiple replicates:}
\label{sec:merg-methc-file} 
When working with a BS-seq project with multiple replicates, you may
first produce a methcounts output file for each replicate individually
and assess the reproducibility of the methylation result by comparing
different replicates. The \prog{merge-methcounts} program is used to
merge the those individual methcounts file to produce a single
estimate that has higher coverage. Suppose you have the three
methcounts files from three different biological replicates,
\fn{ASDF\_R1/R1.meth}, \fn{ASDF\_R2/R2.meth} and
\fn{ASDF\_R3/R3.meth}. To merge those individual methcounts files,
execute
\begin{verbatim}
$ merge-methcounts ASDF_R1/R1.meth  ASDF_R2/R2.meth \
  ASDF_R3/R3.meth -o ASDF.meth
\end{verbatim}

\paragraph{Computation of methylation level statistics}
The \prog{levels} program computes statistics for the output of
\prog{methcounts}. Sample output is below. It computes the total
fraction of cytosines covered, the fraction of cytosines that have
mutated away from the reference, and coverage statistics for both
CpGs and all cytosines. For CpG sites, coverage number reflects taking
advantage of their symmetric nature and merging the coverage on both
strands. For CpG coverage minus mutations, we remove the reads from
CpG sites deemed to be mutated away from the reference. It also computes
average methylation in three different ways, described in Schultz et al.
(2012). This program should provide flexibility to compare methylation data
with publications that calculate averages different ways and illustrate the
variability of the statistic depending on how it is calculated.

\begin{verbatim}
SITES:	1000000
SITES COVERED:	566157
FRACTION MUTATED:	0.001257
FRACTION COVERED:	0.566157
MAX COVERAGE:	439
SYMMETRICAL CpG COVERAGE:	11.3228
SYMMETRICAL CpG COVERAGE (WHEN > 0):	15.3289
SYMMETRICAL CpG COVERAGE (minus mutations):	11.2842
SYMMETRICAL CpG COVERAGE (WHEN > 0) (minus mutations):	15.2768
MEAN COVERAGE:	3.31458
MEAN COVERAGE (WHEN > 0):	5.85452
METHYLATION LEVELS (CpG CONTEXT):
	mean_meth	0.700166
	w_mean_meth	0.667227
	frac_meth	0.766211	
METHYLATION LEVELS (CHH CONTEXT):
	mean_meth	0.0275823
	w_mean_meth	0.0184198
	frac_meth	0.0146346	
METHYLATION LEVELS (CXG CONTEXT):
	mean_meth	0.0217537
	w_mean_meth	0.0170535
	frac_meth	0.00843068	
METHYLATION LEVELS (CCG CONTEXT):
	mean_meth	0.0211243
	w_mean_meth	0.0187259
	frac_meth	0.00630109
\end{verbatim}

To run the \prog{levels} program, execute

\begin{verbatim}
$ levels -o ASDF.levels ASDF.meth
\end{verbatim}

\section{Methylome analysis}
\label{sec:high-level-analys}

The following tools will analyze much of the information about CpG's
generated in previous steps and produce methylome wide profiles of
various methylation characteristics. In the context of Methpipe, these
characteristics consist of hypomethylated regions (HMRs), partially
methylated regions (PMRs), differentially methylated regions between
two methylomes (DMRs), regions with allele-specific methylation (AMRs),
and hydroxymethylation.

\subsection{Hypomethylated and hypermethylated regions (HMRs)}
\label{sec:indent-hypo-methyl}

The distribution of methylation levels at individual sites in a
methylome (either CpGs or non-CpG Cs) almost always has a bimodal
distribution with one peak low (very close to 0) and another peak high
(close to 1). In most mammalian cells, the majority of the genome has
high methylation, and regions of low methylation are typically more
interesting. These are called {\em hypo-methylated regions} (HMRs). In
plants, most of the genome has low methylation, and it is the high
parts that are interesting. These are called {\em hyper-methylated
  regions}. For stupid historical reasons in the Smith lab, we call
both of these kinds of regions HMRs. One of the most important
analysis tasks is identifying the HMRs, and we use the \prog{hmr}
program for this. The \prog{hmr} program uses a hidden Markov model
(HMM) approach using a Beta-Binomial distribution to describe
methylation levels at individual sites while accounting for the number
of reads informing those levels. \prog{hmr} automatically learns the
average methylation levels inside and outside the HMRs, and also the
average size of those HMRs.

\paragraph{Requirements on the data:}
We typically like to have about 10x coverage to feel very confident in
the HMRs called in mammalian genomes, but the method will work with
lower coverage. The difference is that the boundaries of HMRs will be
less accurate at lower coverage, but overall most of the HMRs will
probably be in the right places if you have coverage of 5-8x
(depending on the methylome). Boundaries of these regions are totally
ignored by analysis methods based on smoothing or using fixed-width
windows.

\paragraph{Typical mammalian methylomes:}
Running \prog{hmr} requires a file of methylation levels formatted
like the output of the \prog{methcounts} program (as described
above). The following command will work well for identifying mammalian
HMRs if there is sufficient coverage in the underlying methylomes:
\begin{verbatim}
$ ./hmr -o Human_ESC.hmr Human_ESC.meth
\end{verbatim}
The output will be in BED format, and the indicated strand (always
positive) is not informative. The name column in the output will just
assign a unique name to each HMR. Each time the \prog{hmr} is run it
requires parameters for the HMM to use in identifying the HMRs. We
usually train these HMM parameters on the data being analyzed, since
the parameters depend on the average methylation level and variance of
methylation level; the variance observed can also depend on the
coverage. However, in some cases it might be desirable to use the
parameters trained on one data set to find HMRs in another. The option
\op{-p} indicates a file in which the trained parameters are written,
and the argument \op{-P} indicates a file containing parameters (as
produced with the \op{-p} option on a previous run) to use:
\begin{verbatim}
$ ./hmr -p Human_ESC.hmr.params -o Human_ESC.hmr Human_ESC.meth
$ ./hmr -P Human_ESC.hmr.params -o Human_NHFF_ESC_params.hmr Human_NHFF.meth
\end{verbatim}
In the above example, the parameters were trained on the ESC
methylome, stored in the file \fn{Human\_ESC.hmr.params} and then
used to find HMRs in the NHFF methylome.
This is useful if a particular methylome
seems to have very strange methylation levels through much of the
genome, and the HMRs would be more comparable with those from some
other methylome if the model were not trained on that strange
methylome.

%% Meaning of scores

%% Verbose output

\paragraph{Plant (and similar) methylomes:} 
The plant genomes, exemplified by \textit{A. thaliana}, are devoid of
DNA methylation by default, with genic regions and transposons being
hyper-methylated, which we termed HyperMRs to stress their difference
from \textit{hypo-methylated regions} in mammalian methylomes. DNA
methylation in plants has been associated with expression regulation
and transposon repression, and therefore characterizing HyperMRs is of
much biological relevance. In addition to plants, hydroxymethylation
tends to appear in a small fraction of the mammalian genome, and therefore
it makes sense to identify hyper-hydroxymethylated regions.

The first kind of HyperMR analysis involves finding continuous blocks
of hyper-methylated CpGs with the \prog{hmr} program. Since \prog{hmr}
is designed to find hypo-methylated regions, one needs first to invert
the methylation levels in the \prog{methcounts} output file as
follows:
\begin{verbatim}
$ awk '{$5=1-$5; print $0}' Col0.meth > Col0_inverted.meth
\end{verbatim}
Next one may use the \prog{hmr} program to find ``valleys'' in the
inverted Arabidopsis methylome, which are the hyper-methylated regions
in the original methylome. The command is invoked as below
\begin{verbatim}
$ ./hmr -o Col0.hmr Col0_inverted.meth
\end{verbatim}

This kind of HyperMR analysis produces continuous blocks of
hyper-methylated CpGs. However in some regions, intragenic regions in
particular, such continuous blocks of hyper-methylated CpGs are
separated by a few unmethylated CpGs, which have distinct sequence
preference when compared to those CpGs in the majority of unmethylated
genome. The blocks of hyper-methylated CpGs and gap CpGs together form
composite HyperMRs. The \prog{hypermr} program, which implements a
three-state HMM, is used to identify such HyperMRs. Suppose the
\prog{methcounts} output file is \fn{Col0\_Meth.bed}, to find HyperMRs
from this dataset, run
\begin{verbatim}
$ ./hypermr -o Col0.hypermr Col0.meth
\end{verbatim}
The output file is a 6-column BED file. The first three columns give
the chromosome, starting position and ending position of that
HyperMR. The fourth column starts with the ``hyper:'', followed by the
number of CpGs within this HyperMR. The fifth column is the
accumulative methylation level of all CpGs. The last column indicates
the strand, which is always \lit{+}.

\paragraph{Partially methylated regions (PMRs):}
The \prog{hmr} program also has the option of directly identifying
partially methylated regions (PMRs), not to be confused with partially
methylated domains (see below). These are contiguous intervals where
the methylation level at individual sites is close to 0.5. This should
also not be confused with regions that have allele-specific
methylation (ASM) or regions with alternating high and low methylation
levels at nearby sites. Regions with ASM are almost always among the
PMRs, but most PMRs are not regions of ASM. The \prog{hmr} program is
run with the same input but a different optional argument to find
PMRs:
\begin{verbatim}
$ ./hmr -partial -o Human_ESC.pmr Human_ESC.meth
\end{verbatim}

%%%% Cancer
\paragraph{Giant HMRs observed in cancer samples (AKA PMDs):}

Huge genomic blocks with abnormal hypomethylation have been
extensively observed in human cancer methylomes and more recently
in extraembryonic tissues like the placenta. These domains are
characterized by enrichment in intergenic regions or Lamina 
associated domains (LAD), which are usually hypermethylated in 
normal tissues. Partially methylated domains (PMDs) are not
homogeneously hypomethylated as in the case of HMRs, and contain
focal hypermethylation at specific sites. Hidden Markov Models
can also identify these larger domains. The program \prog{pmd}
is provided for their identification, and can be run as follows:

\begin{verbatim}
$ ./pmd -o Human_ESC.pmd Human_ESC.meth
\end{verbatim}

\subsection{Differential methylation between two methylomes}
\label{sec:differential_methylation}

If you are working with more than one methylome, it may be of interest
to you to identify regions between your methylomes that have
significantly different levels of methylation. To do this, use the
programs \prog{methdiff} and \prog{dmr}. Run \prog{methdiff} first
since its output serves as the input for \prog{dmr}. Since methylation
differences are assessed on a per CpG basis, the methylomes being
compared must come from the same genomes. Otherwise, comparisons will
not be between orthologous CpG's. If you would like to compare
methylomes from different genomes (i.e. human and chimp methylomes),
you must first convert the CpG coordinates for one species into their
orthologous coordinates for the other species. Additionally,
\prog{methdiff} and \prog{dmr} can only compare two methylomes at a
time. Each of these programs is explained in more detail in the
subsections below.

\subsubsection{Differential methylation scores}
\label{sec:methdiff}

The program \prog{methdiff} produces a differential methylation score
for each CpG in a methylome. This score indicates the probability that
the CpG is significantly less methylated in one methylome than the
other. The inputs for \prog{methdiff} are the output of
\prog{methcounts} for each of the two methylomes being analyzed. The
following command calculates differential methylation scores across
two methylomes using the \prog{methdiff} component of Methpipe.
\begin{verbatim}
$ ./methdiff -o Human_ESC_NHFF.methdiff \
             Human_ESC.meth Human_NHFF.meth
\end{verbatim}
So in the output file \fn{Human\_ESC\_NHFF.methdiff} the 5th
column indicates the probability that the methylation level at each
given site is lower in \fn{Human\_NHFF.meth} than in
\fn{Human\_ESC.meth}. For the other direction, you can either
swap the order of those two input files, or just subtract the
probability from 1.0. The method used is due to Altham
(1971)~\cite{altham1969exact}, and is like a one-directional version
of Fisher's exact test. The fourth column of the output file contains
the cytosine's context, and then the number of reads methylated and
total coverage for each of the two samples being compared at that site.

\subsubsection{Differentially methylated regions (DMRs)}
\label{sec:dmr}

Once differential methylation scores have been calculated, the program
\prog{dmr} can be used to identify differentially methylated regions,
or DMRs. DMRs are regions where differential methylation scores
indicate there are many CpGs with a high probability of being
differentially methylated between the two methylomes.
\prog{dmr} uses HMR data from the two methylomes and identifies
a DMR wherever an HMR exists in one methylome but not the other.
It writes the DMRs into two files: one with the HMR in one methylome
and another with the HMR in the other.  It also writes the total number
of CpGs in the DMR and the number of significantly different CpG sites.
The following command finds DMRs using the \prog{dmr} component
of \meth{}:
\begin{verbatim}
$ ./dmr Human_ESC_NHFF.methdiff Human_ESC.hmr \
      Human_NHFF.hmr DMR_ESC_lt_NHFF DMR_NHFF_lt_ESC
\end{verbatim}

\subsection{Allele-specific methylation}

Allele-specific methylation (ASM) occurs when the same cytosine is
differentially methylated on the two alleles of a diploid organism.
ASM is a major mechanism of genomic imprinting, and aberrations can
lead to disease. Included in \prog{methpipe} are three tools to
analyze ASM: \prog{allelicmeth}, \prog{amrfinder}, and \prog{amrtester}.
All of these programs calculate the probability of ASM in a site or
region by counting methylation on reads and analyzing the dependency
between adjacent CpGs, and therefore it is recommended that any samples
analyzed have at least 10$\times$ coverage and 100bp reads for the human
genome.

\subsubsection{Single-site ASM scoring}
\label{sec:allelic_scores}

The program \prog{allelicmeth} calculates allelic-specific methylation
scores for each CpG site. Input files should be the mapped reads files
(\fn{.mr} suffix) produced previously in the mapping step. In the
output file, each row represents a CpG pair made by any CpG and its
previous CpG, the first three columns indicate the positions of the
CpG site, the fourth column is the name including the number of reads
covering the CpG pair, the fifth column is the score for ASM, and the
last four columns record the number of reads of four different
methylation combinations of the CpG pair: methylated methylated (mm),
methylated unmethylated (mu), unmethylated methylated (um), or
unmethylated unmethylated (uu). The following command will calculate
allele-specific methylation scores using the \prog{allelicmeth}
component of Methpipe.

\begin{verbatim}
$ ./allelicmeth -c hg18 -o Human_ESC.allelicmeth Human_ESC.mr
\end{verbatim}

\subsubsection{Allelically methylated regions (AMRs)}

The method described here was introduced in \cite{fang2012genomic}.
The program \prog{amrfinder} scans the genome using a sliding window
to identify AMRs. For a genomic interval, two statistical models are
fitted to the reads mapped, respectively. One model (single-allele
model) assumes the two alleles have the same methylation state, and
the other (two-allele model) represents different methylation states
for the two alleles. Comparing the likelihood of the two models, the
interrogated genomic interval may be classified as an AMR. The input
files must be in the ``epiread'' format, which consists of three
columns. The first column is the chromosome of the read, the second
column is the numbering order of the first CpG in the read, and the
last column is the CpG only sequence of the read.

You can use the program \prog{methstates} to convert the mapped reads
file to the epiread format as below:

\begin{verbatim}
$ ./methstates -c hg18 -o Human_ESC.epiread Human_ESC.mr 
\end{verbatim}

The following command shows an example to run the program
\prog{amrfinder}.
\begin{verbatim}
$ ./amrfinder -o Human_ESC.amr -c hg18 Human_ESC.epiread
\end{verbatim}

There are several options for running \prog{amrfinder}. The \op{-b}
switches from using a likelihood ratio test to BIC as the criterion
for calling an AMR. The \op{-i} option changes the number of
iterations used in the EM procedure when fitting the models.
The \op{-w} option changes the size of the sliding
window, which is in terms of CpGs. The default of 10 CpGs per window
has worked well for us. The \op{-m} indicates the minimum coverage per
CpG site required for a window to be tested as an AMR. The default
requires 4 reads on average, and any lower will probably lead to
unreliable results. AMRs are often fragmented, as coverage fluctuates,
and spacing between CpGs means their linkage cannot be captured by the
model. The \op{-g} parameter is used to indicate the maximum distance
between any two identified AMRS; if two are any closer than this
value, they are merged. The default is 1000, and it seems to work well
in practice, not joining things that appear as though they should be
distinct. In the current version of the program, at the end of the
procedure, any AMRs whose size in terms of base-pairs is less than
half the ``gap'' size are eliminated. This is a hack that has produced
excellent results, but will eventually be eliminated (hopefully
soon).

Finally, the \op{-C} parameter specifies the critical value for
keeping windows as AMRs, and is only useful when the likelihood ratio
test is the used; for BIC windows are retained if the BIC for the
two-allele model is less than that for the single-allele model.
\prog{amrfinder} calculates a false discovery rate to correct for
multiple testing, and therefore most p-values that pass the test
will be significantly below the critical value. The \op{-h} option
produces FDR-adjusted p-values according to a step-up procedure
and then compares them directly to the given critical value, which 
allows further use of the p-values without multiple testing correction.
The \op{-f} omits multiple testing correction entirely by not applying
a correction to the p-values or using a false discovery rate cutoff
to select AMRs.

In addition to \prog{amrfinder}, which uses a sliding window, there is
also the \prog{amrtester} program, which tests for allele-specific
methylation in a given set of genomic intervals. The program can be
run like this:
\begin{verbatim}
$ ./amrtester -o Human_ESC.amr -c hg18 intervals.bed Human_ESC.epiread
\end{verbatim}
This program works very similarly to \prog{amrfinder}, but does not
have options related to the sliding window. This program outputs a
score for each input interval, and when the likelihood ratio test is
used, the score is the $p$-value, which can easily be filtered later.

\subsection{Consistent estimation of hydroxymethylation and methylation
levels}
\label{sec:hydroxy}
If you are interested in estimating hydroxymethylation level and have any
two of \underline{T}et-\underline{A}ssisted \underline{B}isulfite sequencing 
(TAB-seq), oxidative bisulfite sequencing (oxBS-seq) and BS-seq data available,
you can use \prog{mlml} \cite{qu2013mlml} to perform consistent and 
simultaneous estimation.

The input file format could be the default \prog{methcounts} output format
described in Section~\ref{sec:estim-methyl-freq}, or BED format file with 6
columns as the example below:
\begin{verbatim}
chr1    3001345 3001346 CpG:9   0.777777777778  +
\end{verbatim}
Here the fourth column indicates that this site is a CpG site, and the 
number of reads covering this site is 9. The fifth column is the 
methylation level of the CpG site, ranging from 0 to 1. Note that all input
files must be sorted. Assume you have three input files ready: 
\fn{meth\_BS-seq.meth}, \fn{meth\_oxBS-seq.meth} and \fn{meth\_Tab-seq.meth}.
The following command will take all the inputs:
\begin{verbatim}
$ ./mlml -v -u meth_BS-seq.meth -m meth_oxBS-seq.meth \
         -h meth_Tab-seq.meth -o result.txt
\end{verbatim}
If only two types of input are available, e.g. \fn{meth\_BS-seq.meth} and
\fn{meth\_oxBS-seq.meth}, then use the following command:
\begin{verbatim}
$ ./mlml -u meth_BS-seq.meth -m meth_oxBS-seq.meth \
         -o result.txt
\end{verbatim}

In some cases, you might want to specify the convergence tolerance for EM
algorithm. This can be done through \op{-t} option. For example:
\begin{verbatim}
$ ./mlml -u meth_BS-seq.meth -m meth_oxBS-seq.meth \
         -o result.txt -t 1e-2
\end{verbatim}
This command will make the iteration process stop when the difference of
estimation between two iterations is less than $10^{-2}$. The value format 
can be scientific notation, e.g. 1e-5, or float number, e.g. 0.00001.

The output of \prog{mlml} is tab-delimited format. Here is an example:
\begin{verbatim}
chr11	15	16	0.166667	0.19697	0.636364	0 
chr12	11	12	0.222222	0	0.777778	2
\end{verbatim}
The columns are chromosome name, start posistion, end position, 5-mC level, 
5-hmC level, unmethylated level and number of conflicts. To calculate the last
column, a binomial test is performed for each input methylation level 
(can be 2 or 3 in total depending on parameters). If the estimated 
methylation level falls out of the confidence interval calculated from 
input coverage and methylation level, then such event is counted as one 
conflict. It is recommended to filter estimation results based on the number 
of conflicts; if more conflicts happens on one site then it is possible 
that information from such site is not reliable.

\subsection{Computing average methylation level in a genomic interval}
\label{sec:roimethstat}

One of the most common analysis tasks is to compute the average
methylation level through a genomic region. The \prog{roimethstat}
program accomplishes this. It takes a sorted \prog{methcounts} output
file and a sorted BED format file of genomic ``regions of interest''
(hence the ``roi'' in \prog{roimethstat}).  If either file is not
sorted by (chrom,end,start,strand) it can be sorted using the
following command:
\begin{verbatim}
$ LC_ALL=C sort -k 1,1 -k 3,3n -k 2,2n -k 6,6 \
         -o regions_ESC.meth.sorted regions_ESC.meth
\end{verbatim}
From there, \prog{roimethstat} can be run as follows:
\begin{verbatim}
$ ./roimethstat -o regions_ESC.meth regions.bed Human_ESC.meth.sorted
\end{verbatim}
The output format is also 6-column BED, and the score column now takes
the average methylation level through the interval, weighted according
to the number of reads informing about each CpG or C in the
methylation file. The 4th, or "name" column encodes several other
pieces of information that can be used to filter the regions. The
original name of the region in the input regions file is retained, but
separated by a colon (\lit{:}) are, in the following order, (1) the
number of CpGs in the region, (2) the number of CpGs covered at least
once, (3) the number of observations in reads indicating in the region
that indicate methylation, and (4) the total number of observations
from reads in the region. The methylation level is then (3) divided by
(4). Example output might look like:
\begin{verbatim}
chr1  3011124  3015902  REGION_A:18:18:105:166  0.63253  +
chr1  3015904  3016852  REGION_B:5:5:14:31  0.451613  +
chr1  3017204  3017572  REGION_C:2:2:2:9  0.222222  -
chr1  3021791  3025633  REGION_D:10:10:48:73  0.657534  -
chr1  3026050  3027589  REGION_E:2:4:4:32:37  0.864865  -
\end{verbatim}
Clearly if there are no reads mapping in a region, then the
methylation level will be undefined. By default \prog{roimethstat}
does not output such regions, but sometimes they are helpful, and
using the \op{-P} flag will force \prog{roimethstat} to print these
lines in the output (in which case every line in the input regions
will have a corresponding line in the output).

% Example1: ./roimethstat -v -P -o methStats.bed -r regionInterest.bed methFile.bed
% Example2: ./roimethstat -r regionInterest.bed -v -o methStats.bed methFile.bed -P
% Typical Run: ./roimethstat -v methFile.bed -r regionInterest.bed -o methStats.bed

% The output file will also be bed-formatted.  It will be in the
% following format:

% chr#    GenoStart    GenoEnd        name:cpgs:cpgs_with_reads:meth:reads    frac_cpgs_meth    Strand

% The fourth column contains the following information:

% 1) The name of each region
% 2) The number of CpGs in each region
% 3) The number of these CpGs that have at least 1 read
% 4) The total number of methylation counts in this region
% 5) The total number of reads in this region

% To seperate the ':' delimited data into tab delimited data you can
% use one of the following two commands:

% 1) sed 's/:/\t/g' [filename]
% 2) cat [filename] | tr ':' '\t'


\subsection{Computing methylation entropy}
\label{sec:methentropy}

The concept of Entropy was introduced into epigenetics study to characterize the randomness of methylation patterns over several consecutive CpG sites \cite{xie2011}. The\prog{methentropy} program processes epireads and calculates the methylation entropy value in sliding windows of specified number of CpGs. Two input files are required, including the directory containing the chromosome fasta files, and an epiread file as produced by \prog{methstates} program. Use the \op{-w} option to specify the desired number of CpGs in the sliding window; if unspecified, the default value is $4$. In cases where symmetric patterns are considered the same, specify option \op{-F}, this will cause the majority state in each epiread to be forced into ``methylated'', and the minority to ``unmethylated''. The processed epireads will then be used for entropy calculation. To run the program, type command:
\begin{verbatim}
$ ./methentropy -w 5 -v -o Human_ESC.entropy hg18 Human_ESC.epiread
\end{verbatim} 
The output format is the same as \prog{methcount} output. The first 3 columns indicate the genomic location of the center CpG in each sliding window, the 5th column contains the entropy values, and the 6th column shows the number of reads used for each sliding window. Below is an output example.
\begin{verbatim}
chr1	483	+	CpG	2.33914	27
chr1	488	+	CpG	2.05298	23
chr1	492	+	CpG	1.4622	24
chr1	496	+	CpG	1.8784	35
\end{verbatim}




\subsection{Notes on data quality}
\label{sec:notes-data-quality}
The performance of our tools to identify higher-level methylation
features (HMR, HyperMR, PMDs and AMR) depends on the underlying data
quality. One major factor is coverage. Based on our experience, HMR
detection using our method is acceptable above 5x coverage, and we
recommend 10x for reliable results. Our method for identifying
HyperMRs is similar to HMR-finding method, and the above statement
holds. The required coverage therefore is even lower since the
PMD-finding method internally works by accumulating CpGs in
fixed-length bins. We feel $\sim$3x is sufficient. The AMR method
depends on both coverage and read length. Datasets with read length
around 100bp and mean coverage above 10x are recommended for the AMR
method. Another important measure of data quality is the bisulfite
conversion rate. Since most datasets have pretty good bisulfite
conversion rate (above $0.95$), our tools does not explicitly correct
fo the conversion rate.

\section{Methylome visualization}
\label{sec:visualization}


\subsection{Creating UCSC Genome Browser tracks}
\label{sec:browser}

To view the methylation level or read coverage at individual CpG sites
in a genome browser, one needs to create a \lit{bigWig} format file
from a \fn{*\_.meth} file, which is the output of the \prog{methcounts}
program. A methcounts file would look like this:

\begin{verbatim}
chr1	468	469	CpG:30	0.7	+
chr1	470	471	CpG:29	0.931034	+
chr1	483	484	CpG:36	0.916667	+
chr1	488	489	CpG:36	1	+
\end{verbatim}

The first 3 columns shows the physical location of each CpG sites in
the reference genome. The number in the 4th column indicates the
coverage at each CpG site. The methylation level at individual CpG
sites can be found in the 5th column. To create methylation level
tracks or read coverage tracks, one can follow these steps:

\begin{enumerate}
\item Download the \prog{wigToBigWig} program from UCSC genome
  browser's directory of binary utilities
  (\url{http://hgdownload.cse.ucsc.edu/admin/exe/}).
\item Use the \fn{fetchChromSizes} script from the same directory to
  create the \fn{*.chrom.sizes} file for the UCSC database you are
  working with (e.g. hg19). Note that this is the file that is
  referred to as \fn{hg19.chrom.sizes} in step 3.
\item To create a \fn{bw} track for methylation level at single CpG
  sites, convert the methcounts file to bed format using:
\begin{verbatim}
$ awk '{print $1 "\t" $2 "\t" $2+1 "\t" $4":"$6 "\t" $5 "\t" $3}' \
      Human_ESC.meth > Human_ESC.meth.bed
\end{verbatim}
\item To create a \fn{bw} track from the bed format methcounts output, modify and use the following command:
\begin{verbatim}
$ cut -f 1-3,5 Human_ESC.meth.bed | \
     wigToBigWig /dev/stdin hg19.chrom.sizes Human_ESC.meth.bw
\end{verbatim}
  To create a \fn{bw} track for coverage at single CpG sites, modify
  and use the following command:
\begin{verbatim}
$ tr ':' '[Ctrl+v Tab]' < Human_ESC.meth.bed | cut -f 1-3,5 | \
     wigToBigWig /dev/stdin hg19.chrom.sizes Human_ESC.reads.bw
\end{verbatim}
\end{enumerate}
Note that if the \prog{wigToBigWig} or \prog{fetchChromSizes} programs
are not executable when downloaded, do the following:
\begin{verbatim}
$ chmod +x wigToBigWig
$ chmod +x fetchChromSizes
\end{verbatim}

\noindent
You might also want to create \fn{bigBed} browser tracks for HMRs,
AMRs, PMDs, or DMRs. To do so, follow these steps:
\begin{enumerate}
\item Download the \prog{bedToBigBed} program from the UCSC Genome
  Browser directory of binary utilities
  (\url{http://hgdownload.cse.ucsc.edu/admin/exe/}).
\item Use the \fn{fetchChromSizes} script from the same directory to
  create the \fn{*.chrom.sizes} file for the UCSC database you are
  working with (e.g. hg19). Note that this is the file that is
  referred to as \fn{hg19.chrom.sizes} in step 3.
\item Modify and use the following commands: For \fn{*\_HMR.bed} files
  with non-integer score in their 5th column, one needs to round the
  score to integer value, for example:
\begin{verbatim}
$ awk -v OFS="\t" '{print $1,$2,$3,$4,int($5)}' Human_ESC.hmr \
          > Human_ESC.hmr.rounded
\end{verbatim}

\begin{verbatim}
 bedToBigBed Human_ESC.hmr.rounded hg19.chrom.sizes Human_ESC.hmr.bb
\end{verbatim}

  In the above command, since the HMRs are not stranded, we do not
  print the 6th column. Keeping the 6th column would make all the HMRs
  appear as though they have a direction -- but it would all be the
  \lit{+} strand.
\begin{verbatim}
$ bedToBigBed Human_ESC.hmr hg19.chrom.sizes Human_ESC.hmr.bb
\end{verbatim}
\end{enumerate}

% \subsection{Creating custom tracks for direct upload}

% If you do not have access to a web server on which to provide BigBed
% or BigWig files, you can still create tracks to display methylation
% levels. Note that in this case the tracks will be huge and must be
% transferred in their entirety to the remote UCSC Genome
% Browser. Likely this is not an optimal way to be visualizing data.  To
% create a track for methylation level at single CpG sites:
% \begin{verbatim}
% $ cut -f 1-3,5 Human_ESC.meth > Human_ESC.meth.bedGraph
% $ gzip Human_ESC.meth.bedGraph
% \end{verbatim}
% To create a track for coverage at single CpG sites:
% \begin{verbatim}
% $ tr ':' '[Ctrl+v TAB]' < Human_ESC.meth | \
%          cut -f 1-3,5 > Human_ESC.reads.bedGraph
% $ gzip Human_ESC_CpG.reads.bedGraph
% \end{verbatim}
% Note that hitting \lit{Ctrl+v} then hitting \lit{TAB} produces the tab
% character in a terminal window -- not needed when passing this as a
% delimiter to \prog{awk}. These files can be directly uploaded to the
% browser. If track headers are desired, they can be included in the
% \fn{.bedGraph} files before using \prog{gzip}.  
% %%TODO: fix this below
% {\em Need a concise way here to produce tracks for all Cs, not just
%   CpGs.}

% % As for a track for methylation level at all Cs:
% % \begin{verbatim}
% % $ grep - Human_ESC_Cs_methcounts.bed | awk '{print $1"\t"$2"\t"$3"\t"$5*-1}' > temp3;
% % $ grep + Human_ESC_Cs_methcounts.bed | awk '{print $1"\t"$2"\t"$3"\t"$5}' > temp4;
% % $ cat temp3 temp4 | sort | sed -e "1i\\track type="bedGraph" name="Human_ESC_Cs_methcounts" \
% % color=0,128,0 visiblity=full" > Human_ESC_Cs_methcounts.bedGraph> ;
% % rm temp3 temp4
% % gzip Human_ESC_Cs_methcounts.bedGraph
% % \end{verbatim}


\section{Organizing projects with multiple replicates and/or libraries}
\label{sec:organ-datas-with}

Often to build a methylome for a particular population of cells one
obtains multiple biological replicates. For each of these biological
replicates, one may construct more than one bisulfite sequencing
library, and these correspond to technical replicates. Finally, a
given library might be sequenced multiple times, and a given
sequencing run might produce one or more files of reads (for example,
corresponding to different tiles or lanes on an Illumina
sequencer). We organize data to reflect these aspects of the experiment.

\paragraph{Biological replicates}
Distinct biological replicates refer to distinct cells from which DNA
was extracted. Typically these are obtained form different individuals
or different cell cultures. If one is building the ``refernce''
methylome for a given cell type, then eventually these different
biological replicates might be merged. However, the purpose of doing
the replicates separately is so that the biological variation can be
understood. Therefore, certain tests would usually be done prior to
combining the different biological replicates. Fortunately, the
methylomes corresponding to different biological replicates can be
done at much lower coverage, because testing for biological variation
does not require conducting all the kinds of analyses we would want to
conduct on the reference methylome. Distinct biological replicates are
necessarily distinct technical replicates, and so to combine them we
would follow the same procedure as combining technical replicates (see
below). If the name of our methylome is \texttt{ASDF} then we
would organize our biological replicates in a directory structure like
this:
\begin{verbatim}
./ASDF/ASDF_R1/
./ASDF/ASDF_R2/
./ASDF/ASDF_R3/
\end{verbatim}
The properties that we would typically associate with a biological
replicate include the average methylation levels through different
parts of the genome. When comparing biological replicates we might
observe some differences in average methylation levels through
promoters, for example. We would hope and expect these to be minimal,
resulting either from noise due to sampling (when coverage is low) or
variation associated with genotype.

\paragraph{Distinct libraries}
The distinct libraries constitute technical replicates.  Each
technical replicate may have a different distribution of fragment
lengths (as measured using paired-end information or amount of
adaptors present in reads), and possibly a different bisulfite
conversion rate.  Despite these differences, we should generally see
similar methylation levels. If we compare methylation levels between
methylation profiles from different library preparations, using a
defined set of intervals, we would expect to see very little
difference. The difference we do observe should reflect noise due to
sampling in regions with low coverage.  Because methylation data from
two different libraries reflects different actual molecules, combining
data from different libraries is easy. The methylation levels in
\fn{methcounts} files can be combined by simply merging the individual
\fn{methcounts} files.  Similarly, the \fn{methstates} files can be
concatenated and the re-sorted. Within a biological replicate, we
organize libraries as follows.
\begin{verbatim}
./ASDF/ASDF_R1/ASDF_R1_L1/
./ASDF/ASDF_R1/ASDF_R1_L2/
./ASDF/ASDF_R1/ASDF_R1_L3/
\end{verbatim}

\paragraph{Reads files within a library}
For a given library, having a simple pipeline is facilitated by having
a uniform scheme for naming the data files. The following example
illustrates files corresponding to 4 sets of reads, two from
paired-end sequencing and two from single-end sequencing:
\begin{verbatim}
./ASDF/ASDF_R1/ASDF_R1_L1/1_1.fq
./ASDF/ASDF_R1/ASDF_R1_L1/1_2.fq
./ASDF/ASDF_R1/ASDF_R1_L1/2_1.fq
./ASDF/ASDF_R1/ASDF_R1_L1/2_2.fq
./ASDF/ASDF_R1/ASDF_R1_L1/3.fq
./ASDF/ASDF_R1/ASDF_R1_L1/4.fq
\end{verbatim}
The files \fn{1\_1.fq} and \fn{1\_2.fq} are corresponding left and
right mates files. These must be processed together so that the
corresponding mates can be joined. The file \fn{3.fq} is from a
single-end sequencing run. We recommend using symbolic links to set up
these filenames, and to keep subdirectories corresponding to the
individual sequencing runs for the same library.
\begin{verbatim}
./ASDF/ASDF_R1/ASDF_R1_L1/Run1/s_1_1_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/Run1/s_1_2_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/Run2/LID12345_NoIndex_L001_R1_001.fastq
./ASDF/ASDF_R1/ASDF_R1_L1/Run2/LID12345_NoIndex_L001_R2_001.fastq
./ASDF/ASDF_R1/ASDF_R1_L1/Run3/s_1_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/Run3/s_2_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/1_1.fq --> Run1/s_1_1_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/1_2.fq --> Run1/s_1_2_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/2_1.fq --> Run2/LID12345_NoIndex_L001_R1_001.fastq
./ASDF/ASDF_R1/ASDF_R1_L1/2_2.fq --> Run2/LID12345_NoIndex_L001_R2_001.fastq
./ASDF/ASDF_R1/ASDF_R1_L1/3.fq --> Run3/s_1_sequence.txt
./ASDF/ASDF_R1/ASDF_R1_L1/4.fq --> Run3/s_2_sequence.txt
\end{verbatim}
This kind of organization accomplishes four things: (1) it keeps read
files from different runs separate, (2) it provides a place to keep
metadata for the different runs, (3) it allows the reads files to
exist with their original names which might not be different between
runs, and (4) it ensures a simple naming scheme for the data files
needed by our pipeline.

\paragraph{The results directories}
For the example scheme we have been describing, we would organize the
results in the following directories.
\begin{verbatim}
./ASDF/ASDF_R1/ASDF_R1_L1/results/
./ASDF/ASDF_R1/ASDF_R1_L2/results/
./ASDF/ASDF_R1/results/
./ASDF/ASDF_R2/ASDF_R2_L1/results/
./ASDF/ASDF_R2/ASDF_R2_L2/results/
./ASDF/ASDF_R2/results/
./ASDF/ASDF_R3/ASDF_R3_L1/results/
./ASDF/ASDF_R3/ASDF_R3_L2/results/
./ASDF/ASDF_R3/results/
./ASDF/results/
\end{verbatim}


\section{Auxiliary tools}
\label{sec:auxiliary-tools}

\subsection{Count number of lines in a big file}
\label{sec:count-number-lines}

When working with next-generation sequencing data, researchers often
handle very large files, such as FASTQ files containing raw reads
and \fn{*.mr} files containing mapped reads. \prog{lc\_approx} is
an auxiliary tool designed to approximate the number of lines in a
very large file by counting the number of lines in a small, randomly
chosen chunk from the big file and scaling the estimate by file size.
For example, in order to estimate the number of reads in a FASTQ file
\fn{s\_1\_1\_sequence.fq}, run
\begin{verbatim}
$ lc_approx  s_1_1_sequence.fq
\end{verbatim}
It will return the approximate number of lines in this file and by
dividing the above number by 4, you get the approximate number of
reads in that file. The \prog{lc\_approx} can be hundreds of times
faster than the unix tool \prog{wc -l}.

\subsection{Automating methylome analysis}
\label{sec:automate-library-level}
Two bash scripts have been provided to perform quick and consistent analysis
on the individual library level and aggregated biological replicate level.
For each library in a project, \prog{library} sorts, removes duplicate
reads, calculates the bisulfite conversion rate, runs \prog{methcounts}, and
runs \prog{levels}. There is also an option to perform \prog{methcounts}
analysis on all cytosines, which can be toggled by uncommenting the command
in the script. For RRBS data, the \prog{duplicate-remover} command should
be commented.

To run \prog{library}, navigate to the library's results directory --
the structure should look something like
\begin{verbatim}
./ASDF/ASDF_R1/ASDF_R1_L1/results_mm9/
\end{verbatim}
and should contain a single mapped reads file. To run, specify the directory
where \prog{methpipe} binaries can be found, the directory where chromosome
files can be found, and the base name of the library or replicate you are
working on:
\begin{verbatim}
library /home/user/Desktop/methpipe/trunk/bin \
        /home/user/Desktop/mm9_chroms ASDF_R1_L1
\end{verbatim}

When \prog{library} has been run on all individual libraries, the
results can be merged to produce results for the biological replicate using
\prog{merge-methylomes}. This merges bisulfite conversion rate and methcounts
statistics, and then uses them to generate HMRs, PMRs, and AMRs. To run
\prog{merge-methylomes}, navigate to the biological replicate's results
directory -- the structure should look something like
\begin{verbatim}
./ASDF/ASDF_R1/results_mm9/
\end{verbatim}
and should be empty when the program starts. To run, specify the same command
line arguments as above, with the biological replicate as the base name:
\begin{verbatim}
biorep /home/user/Desktop/methpipe/trunk/bin \
       /home/user/Desktop/mm9_chroms ASDF_R1 L
\end{verbatim}

In some cases, it is useful to merge biological replicates rather than
libraries. In this case, the fourth parameter of
\prog{merge-methylomes} should be \op{R}.

All run configurations for the programs in these scripts are
consistent with data in methbase, and therefore direct comparison is
appropriate. These tools should provide a convenient, consistent
workflow for researchers to quickly analyze and compare their
methylomes with those made publically available in methbase.

\subsection{Mapping methylomes between species}
\label{sec:mapp-methyl-betw}
Mapping methylomes between species builds on the the \prog{liftOver}
tool provided by UCSC Genome Browser
\url{http://genome.ucsc.edu/cgi-bin/hgLiftOver}. However it is time
consuming to directly convert large \prog{methcounts} output files
containing millions of CpG sites. We therefore provide the
\prog{fastLiftOver} tool.

\prog{fastLiftOver} requires its own chain files which are generated
with the UCSC \prog{liftOver} program. To generate \prog{fastLiftOver}
chain file from the mouse mm9 genome to the human hg19 genome, you
need a BED file (CpGs-mm9.bed) with all CpG sites in the mouse genome
and the \prog{liftOver} chain file (mm9ToHg19.over.chain) from the
UCSC Genome Browser website. The BED file of mm9 CpG sites follows the
format below, with the fourth column composed of the chromosome name
and the starting location.
\begin{verbatim}
chr1 3000573 3000574 chr1:3000573
chr1 3000725 3000726 chr1:3000725
chr1 3000900 3000901 chr1:3000900
...
\end{verbatim}
First, we will use the UCSC \prog{liftOver} program to map the mouse
CpG sites to the human reference genome:
\begin{verbatim}
liftOver CpGs-mm9.bed mm9ToHg19.over.chain CpGs-mm9Tohg19.bed CpGs-mm9Tohg19-left.bed 
\end{verbatim}
Next, we need to sort the file \prog{CpGs-mm9Tohg19.bed}, that
contains CpG sites mapped to the human reference genome
\begin{verbatim}
export LC_ALL=C;
sort -k1,1 -k2,2n  CpGs-mm9Tohg19.bed -o tmpfile 
mv tmpfile CpGs-mm9Tohg19.bed
\end{verbatim}
The file CpGs-mm9Tohg19.bed will be the \prog{fastLiftOver} chain file
for mapping mouse methcounts output files to the human reference
genome. For example, if we want to map the mouse methcounts file
(Mouse\_ESC.meth) to the human reference genome, we may run the
following command:
\begin{verbatim}
fastLiftOver -f Mouse_ESC.meth -i CpGs-mm9Tohg19.bed -t Mouse_ESC-hg19.meth -v
\end{verbatim}

It is possible that multiple CpG sites in the mouse reference genome
are mapped to the same location in the human reference genome,
therefore the many-to-one conversion problem. In this situation, we
may use the average methylation level weighted by coverage. Please
checkout
\url{https://github.com/songqiang/MethPipe/blob/master/run-fastLiftOver.sh}
for example scripts that go over the whole procedure.


\newpage

\bibliographystyle{plain}
\bibliography{methpipe-manual}

\end{document}
